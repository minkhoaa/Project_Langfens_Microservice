services:

  whisper-asr-alpine:
    command: ["--model","/app/models/ggml-tiny.en.bin",
              "--host","0.0.0.0","--port","8080",
              "--inference-path","/v1/audio/transcriptions",
              "--processors","6","--threads","8"]  
    environment:
      OMP_NUM_THREADS: "8"
      OMP_PROC_BIND: "close"
      OMP_PLACES: "cores"
    build:
      context: .
      dockerfile: Dockerfile
      args:
        MODEL: tiny.en
  
    ports:
      - "2080:8080"

  faster-whisper-cpu:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: faster-whisper-cpu
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Model CTranslate2 trên HF; có thể đổi:
      # tiny / base / small / medium / large-v2 / large-v3 …
      # Khuyến nghị: "Systran/faster-distil-whisper-large-v3" (nhanh/chính xác cân bằng)
      WHISPER__DEFAULT_MODEL: "Systran/faster-distil-whisper-large-v3"
      # Cố định ngôn ngữ (giảm thời gian autodetect). Để trống nếu muốn auto:
      WHISPER__DEFAULT_LANGUAGE: "en"
      # Tùy chọn: số workers nội bộ (song song yêu cầu)
      WHISPER__WORKERS: "2"
    volumes:
      # cache models để lần sau không tải lại
      - ~/.cache/huggingface:/root/.cache/huggingface

  # ===== GPU (NVIDIA) =====
  
  
  faster-whisper-cuda:
    image: fedirz/faster-whisper-server:latest-cuda
    container_name: faster-whisper-cuda
    ports: ["8001:8000"]
    environment:
      WHISPER__DEFAULT_MODEL: "tiny.en"           # hoặc "small" / "tiny" / "tiny.en"
      WHISPER__COMPUTE_TYPE: "int8_float16"    # (nhẹ VRAM hơn FP16), có thể thử "int8"
      WHISPER__WORKERS: "1"                    # tránh nhân đôi model trong RAM/VRAM
      CUDA_VISIBLE_DEVICES: "0"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    gpus: all
